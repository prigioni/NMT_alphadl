# Recurrent seq2seq neural machine translation model

This sub-repository has implemented the basic seq2seq NMT model. 

**[seq2seq](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)**, vanilla seq2seq model without attention mechanism implemented with [Pytorch](https://pytorch.org/).   


**[seq2seq_att](https://arxiv.org/pdf/1409.0473.pdf)**, which is an attention mechanism assembled version.
